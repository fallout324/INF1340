# -*- coding: utf-8 -*-
"""1340 Final Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14erMSjoX3lHCaQbWt28N2fqnY8xEkpFZ

**INF1340 Final Project: Restaurant Review** \
**Student Name: Michael Song, Yiwen Mei, Yuyang Liu**

**Input File for Analysis**
"""

import pandas as pd
df = pd.read_csv('Restaurant reviews.csv')
df

"""**Data Preparation and Data Cleaning**"""

df.info()

"""As we can see, the column $7514$ has only one non-null value. We need to drop it."""

df.drop(['7514'], axis=1, inplace=True)

df.info()

"""Now check for duplicates, drop if any"""

df.duplicated().sum()

df.drop_duplicates(inplace=True)
df.duplicated().sum()

"""Check for werid value"""

df['Rating'].value_counts()

# Replace werid values and finish data cleaning
df['Rating'] = df['Rating'].replace(['Like'], '5')
df['Rating'] = df['Rating'].astype(float)

"""# **Descriptive Analysis**

**Task 1: Calculate the average customer rating for each restaurant and rank them in order**
"""

# Calculate the average customer rating for each restaurant
average_ratings = df.groupby('Restaurant')['Rating'].mean().reset_index()
average_ratings

# Rank the restaurant in order
sorted_ratings = average_ratings.sort_values(by='Rating', ascending=False)
sorted_ratings

# Find top 10 best liked restaurants based on average rating
sorted_ratings.head(10)

# Find top 10 least liked restaurants based on average rating
sorted_ratings.tail(10)

# Find histogram of all ratings distribution
import matplotlib.pyplot as plt

plt.hist(df['Rating'], bins=5, edgecolor='black')
plt.xlabel('Rating score')
plt.ylabel('Frequency')
plt.title('Distribution of All Ratings')

# Find histogram of average ratings distribution
plt.hist(average_ratings['Rating'], bins=10, edgecolor='black')
plt.xlabel('Rating score')
plt.ylabel('Frequency')
plt.title('Distribution of Average Ratings')

"""**Calculate the average pictures posted along with the review text for each restaurant.**"""

# Find pictures posted for each restaurant
average_pictures = df.groupby('Restaurant')['Pictures'].mean().reset_index()
average_pictures

# Rank the restaurant in order from greatest number of pictures to the least
sorted_pictures = average_pictures.sort_values(by='Pictures', ascending=False)
sorted_pictures

# Find histogram of average pictures distribution
plt.hist(average_pictures['Pictures'], bins=10, edgecolor='black')
plt.xlabel('Number of pictures')
plt.ylabel('Frequency')
plt.title('Distribution of Average Posted Pictures')

"""**Find number of positive and negative reviews each restaurant have.**"""

# Add a new column named "Label", positive for rating greater than or equal to 3 and negative for rating less than 3
df['Label'] = ['positive' if x >= 3 else 'negative' for x in df['Rating']]
df

# Grouping the reviews by positive and negative, construct word cloud for both groups
df_positive_reviews = df[df['Label'] == 'positive']
df_negative_reviews = df[df['Label'] == 'negative']
df_positive_reviews['Review'] = df_positive_reviews['Review'].fillna('')
df_negative_reviews['Review'] = df_negative_reviews['Review'].fillna('')

# Count the number of positive and negative reviews for each restaurant
reviews_counts = df.groupby(['Restaurant', 'Label']).size().unstack(fill_value=0)
reviews_counts

"""**Find word clouds for both positive and negative reviews. Also look for frequently occuring words so we know what words are associated with higher or lower ratings.**"""

# Word cloud for positive group
from wordcloud import WordCloud
positive_reviews = ''.join(df_positive_reviews['Review'])
wordcloud_positive = WordCloud(width=800, height=400, background_color='white').generate(positive_reviews)

plt.figure(figsize=(10, 5))
plt.imshow(wordcloud_positive)
plt.title('Word Cloud for Positive Reviews')
plt.axis('off')
plt.show()

# Checking for word frequency in positive reviews
import nltk
from nltk import word_tokenize
nltk.download('punkt')
tokens_positive = word_tokenize(positive_reviews)

words1_positive = [token for token in tokens_positive if token.isalpha()]
words2_positive = [word.lower() for word in words1_positive]

nltk.download('stopwords')
from nltk.corpus import stopwords
stopWords = stopwords.words('english')
words3_positive = [word for word in words2_positive if word not in stopWords]

nltk.download('averaged_perceptron_tagger')
words_tags_positive = nltk.pos_tag(words3_positive)
noun_words_positive = [word for (word,tag) in words_tags_positive if tag.startswith('N')]

noun_word_freq_positive = nltk.FreqDist(noun_words_positive)
noun_word_freq_positive

# Plot noun words frequency
noun_word_freq_positive.plot(30)

# Also look for adjectives that used in the positive reviews
words_tags_positive = nltk.pos_tag(words3_positive)
adj_words_positive = [word for (word,tag) in words_tags_positive if tag.startswith('J')]

adj_word_freq_positive = nltk.FreqDist(adj_words_positive)
adj_word_freq_positive

# Plot adjective words frequency
adj_word_freq_positive.plot(30)

# Word cloud for negative group
negative_reviews = ''.join(df_negative_reviews['Review'])
wordcloud_negative = WordCloud(width=800, height=400, background_color='white').generate(negative_reviews)

plt.figure(figsize=(10, 5))
plt.imshow(wordcloud_negative)
plt.title('Word Cloud for Negative Reviews')
plt.axis('off')
plt.show()

# Checking for word frequency in negative reviews
tokens_negative = word_tokenize(negative_reviews)

words1_negative = [token for token in tokens_negative if token.isalpha()]
words2_negative = [word.lower() for word in words1_negative]

stopWords = stopwords.words('english')
words3_negative = [word for word in words2_negative if word not in stopWords]

words_tags_negative = nltk.pos_tag(words3_negative)
noun_words_negative = [word for (word,tag) in words_tags_negative if tag.startswith('N')]

noun_word_freq_negative = nltk.FreqDist(noun_words_negative)
noun_word_freq_negative

# Plot noun words frequency
noun_word_freq_negative.plot(30)

# Also check for negative adjective words
words_tags_negative = nltk.pos_tag(words3_negative)
adj_words_negative = [word for (word,tag) in words_tags_negative if tag.startswith('J')]

adj_word_freq_negative = nltk.FreqDist(adj_words_negative)
adj_word_freq_negative

# Plot adjective words frequency
adj_word_freq_negative.plot(30)

"""# **Diagnostic Analysis**"""

# Sentiment Analysis: Want to know whether sentiment is correlated with ratings
df.head()

# Import sentiment analysis relevant packages and functions
from nltk.sentiment import SentimentIntensityAnalyzer
nltk.download('vader_lexicon')
analyzer = SentimentIntensityAnalyzer()

compound_list = []
for index, row in df.iterrows():
   text = row.Review
   scores = analyzer.polarity_scores(str(text))
   compound = scores['compound']
   compound_list.append(compound)

# Define a function to determine polarity scores for reviews, and return the compound score
def compound_score(text):
   scores = analyzer.polarity_scores(str(text))
   return scores['compound']

# Apply the function to the reviews and create a new column to incorporate compound scores
df['Compound'] = df['Review'].apply(compound_score)
df

# Find average polarity score for each restaurant
average_compound = df.groupby('Restaurant')['Compound'].mean().reset_index()
average_compound

# Find histogram of average compound scores distribution
plt.hist(average_compound['Compound'], bins=10, edgecolor='black')
plt.xlabel('Compound Scores')
plt.ylabel('Frequency')
plt.title('Distribution of Average Compound Scores')

# Return true if compound score > 0 and Label = positive, false if compound score < 0 and Label = negative
df['Condition Checking'] = (df['Label'] == 'positive') & (df['Compound'] > 0)

# Check how many observations satisfy the criteria (How many return true?).
# By doing so we can check if our label assigning process matches with the sentiment score.
condition_count = df['Condition Checking'].value_counts()

print("Count of conditions met:", condition_count[True])
print("Count of conditions unmet:", condition_count[False])

# Rating Discrepancy Analysis: Identify restaurants with a significant discrepancy between the average rating and sentiment of reviews.
# Data Aggregation: Group the data by 'restaurant_name' and calculate the average rating for each restaurant.
average_ratings = df.groupby('Restaurant')['Rating'].mean().reset_index()
average_ratings

# Find the absolute difference between the average rating and sentiment scores for each restaurant.
# Hightlight restaurants where there is a notable gap between numerical ratings and the sentiment expressed in reviews.
df['Difference'] = abs(average_ratings['Rating'] - average_compound['Compound'])

# Rank the restaurants based on the calculated discrepancy so we can identify the top restaurants with the largest gaps.
sorted_diff = df.sort_values(by='Difference', ascending=False)
sorted_diff.head(50)

# Does correlation analysis by analyzing relationship between length of review text and rating
df_correlation = df.drop(columns=['Restaurant', 'Reviewer', 'Metadata', 'Time', 'Pictures'])
df_correlation['Review']  = df_correlation['Review'].str.len()
ax1 = df_correlation.plot.scatter(x='Review', y='Rating', c='DarkBlue')

"""From the above plot, it seems that longer reviews have a correlation with higher ratings, although there is fluctuation with the less common decimal ratings.

"""

# Does regression analysis by analyzing relationship between day of the week and number of reviews
df_regression = df.drop(columns=['Restaurant', 'Reviewer', 'Review', 'Rating', 'Pictures'])
df_regression[['Date', 'Time']] = df_regression['Time'].str.split(' ', expand=True)
df_regression[['Reviews', 'Metadata']] = df_regression['Metadata'].str.split(' , ', expand=True)
df_regression[['Reviews', 'Metadata']] = df_regression['Reviews'].str.split(' ', expand=True)

# Create a DataFrame with a single date string
df_regression1 = df_regression.drop(columns=['Time', 'Reviews', 'Metadata'])
date_obj = df_regression1

# Convert the date string to a datetime object using the to_datetime() function
date_obj['Date'] = pd.to_datetime(date_obj['Date'])

# Add a new column to the DataFrame to store the day of the week
date_obj['Day'] = date_obj['Date'].dt.day_name()

# Format data
df_regression['Date'] = date_obj['Day']
df_regression = df_regression.drop(columns=['Time', 'Metadata'])
df_regression['Reviews'] = df_regression['Reviews'].astype(float)

# Initialize variables
sat_count = 0
sun_count = 0
mon_count = 0
tue_count = 0
wed_count = 0
thu_count = 0
fri_count = 0

for index, row in df_regression.iterrows():
  if df_regression['Date'][index] == 'Saturday':
    sat_count = sat_count + df_regression['Reviews'][index]
  elif df_regression['Date'][index] == 'Sunday':
    sun_count = sun_count + df_regression['Reviews'][index]
  elif df_regression['Date'][index] == 'Monday':
    mon_count = mon_count + df_regression['Reviews'][index]
  elif df_regression['Date'][index] == 'Tuesday':
    tue_count = tue_count + df_regression['Reviews'][index]
  elif df_regression['Date'][index] == 'Wednesday':
    wed_count = wed_count + df_regression['Reviews'][index]
  elif df_regression['Date'][index] == 'Thursday':
    thu_count = thu_count + df_regression['Reviews'][index]
  elif df_regression['Date'][index] == 'Friday':
    fri_count = fri_count + df_regression['Reviews'][index]

data = {'Day': ['Saturday', 'Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday'],
        'Reviews': [sat_count, sun_count, mon_count, tue_count, wed_count, thu_count, fri_count]}

# Create graph to represent data
df_final_regression = pd.DataFrame(data)
ax = df_final_regression.plot.bar(x='Day', y='Reviews', rot=0)

"""We see more reviews on weekends compared to weekdays."""

# Does sentiment analysis relative to rating

ax1 = df.plot.scatter(x='Compound', y='Rating', c='DarkBlue')

"""There are more positive sentiment correlates with better ratings."""

# Correlates number of pictures relative to rating
ax1 = df.plot.scatter(x='Pictures', y='Rating', c='DarkBlue')

"""We find that people are much more likely to take pictures for food when they are planning on leaving a good review.

**Predictive Analysis About 3-4 Tasks**
"""

# convert positive as 1, negative as 0
df_new = df[['Review','Label']].copy()
df_new['Label'] = [1 if x == 'positive' else 0 for x in df_new['Label']]
df_new

df_new.info()

"""Drop Null Values in the reviews"""

df_new.dropna(inplace=True)
df_new.info()

# create a function to clean the review
def clean_review(review):
    tokens = word_tokenize(review)
    stops = set(stopwords.words('english'))
    meaningful_word = [w.lower() for w in tokens if w.lower() not in stops]
    return ' '.join(meaningful_word)

df_new['clean'] = df_new['Review'].astype(str).apply(lambda x: clean_review(x))
df_new

# split the train and test data
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer

train_data, test_data = train_test_split(df_new,test_size=0.2, random_state=10086)
v = CountVectorizer(analyzer='word')
x_train_token = v.fit_transform(train_data['clean'])
x_test_token = v.transform(test_data['clean'])
y_train = train_data['Label']
y_test = test_data['Label']

!pip3 install catboost

from catboost import CatBoostClassifier
from sklearn.metrics import accuracy_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from xgboost import XGBClassifier
from sklearn.linear_model import SGDClassifier

# define a function for automatic training of different models
def train_model(models, name, train, train_label, test, test_label):
    pred_result = []
    for i, model in enumerate(models):
        fitted = model.fit(train, train_label)
        pred = fitted.predict(test)
        accuracy = round(accuracy_score(pred, test_label)*100, 3)
        pred_result.append(pred)
        print(name[i]+' has a '+str(accuracy)+'% of accuracy')
    return pred_result

# fit the model
models = [CatBoostClassifier(verbose=False),
          RandomForestClassifier(),
          AdaBoostClassifier(),
          XGBClassifier(),
          SGDClassifier()]
name = ['CatBoost', 'Random Forest', 'AdaBoost', 'XgBoost', 'SGD']

predicted_result = train_model(models, name, x_train_token, y_train, x_test_token, y_test)

# plot out confusion matrix for visualization
from sklearn.metrics import confusion_matrix
from sklearn import metrics

def plot_confusion_matrix(true_label, predicted_result, name):
    for i in range(len(name)):
        confu = confusion_matrix(true_label, predicted_result[i])
        cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confu, display_labels = ['Negative', 'Positive'])
        cm_display.plot()
        plt.title('Confusion Matrix Plot for '+name[i])
        plt.show()

plot_confusion_matrix(y_test, predicted_result, name)

"""**Prescriptive Analysis about 2 tasks**

**Optimizing Engagement Based on Reviewer's Influence:**

Now let's pretending we are restaurant owners. We will evaluate the impact of each reviewer based on their follower numbers, and then develop strategies to engage with these influential reviewers, such as inviting them for special tasting events or requesting them to review new dishes.
"""

# re-read the data file since we have already done some transformation on the original one
df = pd.read_csv('Restaurant reviews.csv')

import re
# Re-extracting number of followers from the Metadata column
def extract_followers(metadata):
    if not isinstance(metadata, str):
        return 0
    followers = re.search(r'(\d+) Follower', metadata)
    if followers:
        return int(followers.group(1))
    else:
        return 0

# show the distribution of reviewers' followers
df = pd.read_csv('Restaurant reviews.csv')
df['Followers'] = df['Metadata'].apply(extract_followers)
df['Followers'].describe()

# plot out a histogram to show the right skewness
plt.hist(df['Followers'])
plt.xlabel('Followers')
plt.ylabel('Frequency')
plt.title('Distribution of Reviewer\'s Followers')

# show the list of top reviwers
top_reviewers = df.sort_values(by='Followers', ascending=False).drop_duplicates(subset='Reviewer').head(10)[['Reviewer', 'Followers']]
top_reviewers.reset_index(drop=True, inplace=True)
top_reviewers

"""The above 10 reviewers have the highest followers and thus the most fan bases. The resturant owners should encourage them to visit again and review new or improved offerings. Their positive reviews could have a more substantial impact on attracting new customers due to their wider reach.

**Personalized Suggestion for each Restaurant**

In this part, we will list out the top 10 words for each restaurant, with their reviews' average rating and sentiment score on the side. By reading at the final dataframe, the restaurant owner can easily understand what the customers like or don't like about, and improve accordingly.
"""

# clean and return the polarity score of each review
from textblob import TextBlob

def analyze_sentiment(review):
    review = re.sub('[^a-zA-Z]', ' ', str(review))
    review = review.lower()
    return TextBlob(review).sentiment.polarity

# Applying the function to the review column
df['Sentiment'] = df['Review'].apply(analyze_sentiment)

# exclude some words of choice, and all tag with PRP, CC, IN, UH in nltk labelling
from nltk.stem import PorterStemmer
from nltk import pos_tag
import string
from collections import Counter

def clean_and_tokenize(text):
    if not isinstance(text, str):
        return []
    stemmer = PorterStemmer()
    text = text.translate(str.maketrans('', '', string.punctuation)).lower()

    tokens = word_tokenize(text)

    secret_list = ['food', 'good', 'best', 'really', 'visit', 'place', 'nice', 'great', 'one', 'us', 'also','awesome'
                'absolute', 'excellent', 'amazing']
    filtered_tokens = [word for word in tokens if word not in stopwords.words('english')
                       and word not in secret_list]
    tagged_tokens = pos_tag(filtered_tokens)
    return [stemmer.stem(word) for word, tag in tagged_tokens if tag not in ['PRP', 'CC', 'IN', 'UH']]

# Apply the function to each review
df['Cleaned_Review'] = df['Review'].apply(clean_and_tokenize)

# Group by restaurant and concatenate all reviews
grouped_reviews = df.groupby('Restaurant')['Cleaned_Review'].sum()

# Find the top 10 most common words for each restaurant
top_words_per_restaurant = {restaurant: Counter(reviews).most_common(10) for restaurant, reviews in grouped_reviews.items()}

# Converting the top words per restaurant into a DataFrame for better presentation
top_words_df = pd.DataFrame.from_dict(top_words_per_restaurant, orient='index')

top_words_df_full = top_words_df.reset_index()
top_words_df_full.rename(columns={'index': 'Restaurant'}, inplace=True)

# Calculate sentiment score for each review
df['Sentiment_Score'] = df['Review'].apply(analyze_sentiment)

# Replace weird values and finish data cleaning
df['Rating'] = df['Rating'].replace(['Like'], '5')
df['Rating'] = df['Rating'].astype(float)

# Group by restaurant and calculate the average rating and sentiment score
average_scores = df.groupby('Restaurant').agg({'Rating':'mean', 'Sentiment_Score':'mean'})

# merge the data frame on restaurant name, and rename some column names
merge_df = pd.merge(average_scores, top_words_df_full, on='Restaurant')
merge_df = merge_df.rename(columns={'Rating': 'Average Rating',
            'Review': 'Review Counts', '0': 'Top 1'})
new_column_names = {i: f'Top {i+1} Word' for i in range(10)}
merge_df = merge_df.rename(columns=new_column_names)

merge_df.head()

"""Having obtained the table, let's consider an example: at 10 Downing Street, the top 10 words include ambience, beer, service, order, time, chicken, drink, go, staff, and floor. This suggests that the restaurant is renowned for its inviting ambiance and a selection of quality beers. The mentions of service and staff indicate that customers appreciate the friendly and efficient service. Such insights from the table can be invaluable for restaurant owners, offering a clear perspective on areas for enhancement and strengths to maintain. This data-driven approach enables restaurant managers to tailor their improvements effectively, ensuring that the aspects customers value most are preserved and enhanced."""